{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Computing\n",
    "\n",
    "## General thoughts\n",
    "\n",
    "Parallel computing is a programming method that **harnesses the power of multiple processors (typically CPU cores) at once**.\n",
    "\n",
    "There are many types of parallelism, some of which are (from micro to macro)\n",
    "\n",
    "* **Instruction level parallelism** (e.g. SIMD)\n",
    "* **Multi-threading** (shared memory)\n",
    "* **Multi-processing** (shared system memory)\n",
    "* **Distributed processing** (typically no shared memory)\n",
    "\n",
    "**Import note before we start: At the center of an efficient parallel code is a fast serial code!!**\n",
    "\n",
    "### Why Go Parallel?\n",
    "\n",
    "<img src=\"../../static/42-years-processor-trend.svg\" width=700px>\n",
    "\n",
    "Interesting video on the topic of \"The Future of Microprocessors\" https://www.youtube.com/watch?v=zX4ZNfvw1cw (coincidentally from Juliacon :P)\n",
    "\n",
    "### When to Go Parallel?\n",
    "\n",
    "- If parts of your (optimized!) serial code aren't fast enough.\n",
    "  - There are costs: parallelization typically increases the code complexity\n",
    "- If your system has multiple execution units (CPU threads, GPU threads, ...).\n",
    "  - Import on supercomputers, but also on modern desktop computers and laptops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Do I Have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Hwloc\n",
    "Hwloc.num_physical_cores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there may be more than one CPU thread per physical CPU core (e.g. hyperthreading)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sys.CPU_THREADS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does Maxwell Have?\n",
    "\n",
    "The [Maxwell Infrastructure](https://confluence.desy.de/display/MXW/Infrastructure) page summarises the hardware:\n",
    "\n",
    "| Compute Hardware                          |               | Infiniband Hardware |        | Storage     |        |\n",
    "|-------------------------------------------|---------------|---------------------|--------|-------------|--------|\n",
    "| CPU+GPU nodes                             | 798           | root switches       | 6      | GPFS exfel  | ~40 PB |\n",
    "| Total number of cores with hyperthreading | 61696         | top switches        | 12     | GPFS petra3 | ~20 PB |\n",
    "| Total number of PHYSICAL cores            | 30898         | leaf switches       | 42     | BeeGFS desy | 1.5 PB |\n",
    "| Theoretical CPU peak performance          | 1074 TFlops   | IB cables (#)       | >1432  | BeeGFS cssb | 3.2 PB |\n",
    "| Total RAM                                 | 420 TB        | IB cables (length)  | >7.6km |             |        |\n",
    "| GPU nodes                                 | 180           |                     |        |             |        |\n",
    "| Total number of GPUs                      | 379           |                     |        |             |        |\n",
    "| Theoretical GPU peak performance          | 2330 TFlops   |                     |        |             |        |\n",
    "| Total peak performance                    |  3404 TFlops1 |                     |        |             |        |\n",
    "\n",
    "There are two main kinds of nodes on Maxwell:\n",
    "\n",
    "| HT Cores | Cores | CPUs | CPU           |\n",
    "|----------|-------|------|---------------|\n",
    "| ~160     | ~20   | 2x   | Intel E5-2698 |\n",
    "|  256     |  64   | 2x   | AMD EPYC 7542 |\n",
    "\n",
    "Note that:\n",
    "\n",
    "- Few different types of Intel CPUs, between 18 and 20 cores/cpu\n",
    "- Hyperthreaded cores = 2 (physical CPUs) * 64 (cores/CPU) * 2 (threads/core) = 256 HT Cores for EPYC, similar for Intel\n",
    "\n",
    "Even if you only use a single node you have access to 128 CPU cores (64 per CPU). Hence, if you would use only a single core, the node utilization would be less than 1%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amdahl's Law\n",
    "\n",
    "Naive strong scaling expectation: I have 4 cores, give me my 4x speedup! However that is not the case:\n",
    "\n",
    "> The overall performance improvement gained by optimizing a single part of a system is limited by the fraction of time that the improved part is actually used\n",
    "\n",
    "More formally:\n",
    "\n",
    ">If $p$ is the fraction of a code that can be parallelized than the maximal theoretical speedup by parallelizing on $n$ cores is given by\n",
    ">$$ F(n) = 1/(1-p + p/n) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "F(p, n) = 1 / (1 - p + p / n)\n",
    "\n",
    "pl = plot()\n",
    "for p in reverse(sort(vcat(0.2:0.2:1, [0.9, 0.95])))\n",
    "    plot!(pl, n -> F(p, n), 1:16, lab=\"$(Int(p*100))%\", lw=2,\n",
    "        legend=:topleft, xlab=\"number of cores\", ylab=\"parallel speedup\", frame=:box)\n",
    "end\n",
    "pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Parallel Computing](https://docs.julialang.org/en/v1/manual/parallel-computing/) in Julia\n",
    "\n",
    "Julia provides support for all types of parallelism mentioned above (same order)\n",
    "\n",
    "* `@simd`, [SIMD.jl](https://github.com/eschnett/SIMD.jl), [LoopVectorization.jl](https://github.com/JuliaSIMD/LoopVectorization.jl)\n",
    "* `Threads.@threads`, `Threads.@spawn`, [FLoops.jl](https://github.com/JuliaFolds/FLoops.jl), [ThreadsX.jl](https://github.com/tkf/ThreadsX.jl) ...\n",
    "* `@spawnat`, `@fetch`, `RemoteChannel`, `SharedArray`\n",
    "* `@spawnat`, `@fetch`, `RemoteChannel`, [DistributedArrays.jl](https://github.com/JuliaParallel/DistributedArrays.jl), [MPI.jl](https://github.com/JuliaParallel/MPI.jl)\n",
    "\n",
    "With supercomputing in mind, we will start by focusing on multi-process parallelism which allows us to utilize multiple cores on the same or different nodes/machines (distributed computing).\n",
    "\n",
    "But before we do, it's instructive to take a closer look at **tasks**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "By default, Julia waits for every command to finish (\"**blocking**\") and run everything sequentially.\n",
    "\n",
    "**Tasks** are a control flow feature that allows computations to be suspended and resumed in a flexible manner to implement **cooperative multitasking**. (This feature is sometimes called by other names, such as coroutines, green-, or lightweight threads.)\n",
    "\n",
    "Tasks are managed by Julia and can be run in a **concurrent** fashion.\n",
    "\n",
    "> **Concurrency** means executing multiple tasks at the same time but not necessarily simultaneously.\n",
    "\n",
    "An important use case is **asynchronous I/O**, which is typically slow. Examples are:\n",
    "\n",
    "- **multiple user input** (Why not already process some of the input?)\n",
    "- **data dumping to disk** (Maybe it's possible to continue a calculation?)\n",
    "- **receiving calculations from worker processes**\n",
    "\n",
    "## `@async` and `@sync`\n",
    "\n",
    "We can create and schedule a task for asynchronous execution with the [`@async` macro](https://docs.julialang.org/en/v1/base/parallel/#Base.@async).\n",
    "\n",
    "What this means is that for whatever falls into its scope, Julia will start a task to then proceed to whatever comes next in the script without waiting for the task to complete (\"**non-blocking**\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time sleep(2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time @async sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia allows the script to proceed (and the `@time` macro to fully execute) without waiting for the task (in this case, sleeping for two seconds) to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the partner macro `@sync` to synchronize, that is wait for all encapsulated tasks. (see `?@sync`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time @sync @async sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, here it doesn't make much sense to write `@sync @async` - we could simply drop it altogether. A better example is the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time @sync begin\n",
    "    @async sleep(2.0)\n",
    "    @async sleep(2.0)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = rand(1000, 1000)\n",
    "B = rand(1000, 1000)\n",
    "\n",
    "@time t = @async A * B;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time A * B;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function io_bound_task()\n",
    "    sleep(5.0)\n",
    "    return true\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time my_io_bound_task = @async io_bound_task()\n",
    "\n",
    "@time fetch(my_io_bound_task)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "julia 1.8.4",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
